{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NICAR 2020: NLP Workshop\n",
    "\n",
    "Spacy is a library that can assist you in doing linguistic analyses. \n",
    "\n",
    "To install and use the Englis-language version of spacy you should run these commands in your virtual environment:\n",
    "`pip3 install spacy`\n",
    "`python3 -m spacy download en_core_web_sm`\n",
    "We will be importing the `text.txt` file in our `data` folder. It contains a sample article about a very special [cat](https://www.buzzfeednews.com/article/juliareinstein/this-thicc-lazy-high-maintenance-incredibly-well-hydrated/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our libraries\n",
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2990"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# opens the text file and turns it into a string\n",
    "text = open(\"../data/text.txt\",\"r+\").read()\n",
    "len(text) # this returns the length of characters and spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus & Tokenizing\n",
    "\n",
    "Now let's turn the string into a corpus for spacy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document can act like a list of words. To access each word or 'token' we can use the built in function `.text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can count some words by:\n",
    "- turning the words into a list\n",
    "- turning that list into a pandas data frame\n",
    "- counting the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for token in doc:\n",
    "    rows.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dataframe = pd.DataFrame(rows)\n",
    "word_dataframe.columns = ['word']\n",
    "word_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two ways to count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = word_dataframe['word'].value_counts().reset_index()\n",
    "word_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_alt = word_dataframe.groupby('word').agg({\"word\":\"count\"})\n",
    "word_count_alt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save the outputs by using `.to_csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count.to_csv('../output/word_count.csv', index=False)\n",
    "word_count_alt.to_csv('../output/word_count2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Ways to Filter & Clean\n",
    "\n",
    "When you convert a doc into tokens using spacy, it doesn't just contain information about each word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords\n",
    "\n",
    "Spacy has a built-in list of 'stopwords', or extremely common words that might not add much insight to our analysis. When you tokenize your document, it also checks your tokens against the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for token in doc:\n",
    "    rows.append([token.text, token.is_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_dataframe = pd.DataFrame(rows)\n",
    "stop_dataframe.columns = ['word', 'is_stop']\n",
    "stop_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the difference between the aggregated counts with and without the stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stop = stop_dataframe[~stop_dataframe['is_stop']]\n",
    "\n",
    "no_stop['word'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_dataframe['word'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "When you tokenize your document with spacy, it also _lemmatizes_ them, or groups words and their derivatives together (e.g., _organize_, _organized_, and _organizing_). For instance:\n",
    "\n",
    "- am, are, is $\\Rightarrow$ be\n",
    "- car, cars, car's, cars' $\\Rightarrow$ car\n",
    "\n",
    "You can access a token's lemmatized word by using the `token.lemma_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for token in doc:\n",
    "    rows.append([token.text, token.lemma_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This</td>\n",
       "      <td>this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bruno</td>\n",
       "      <td>Bruno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>he</td>\n",
       "      <td>-PRON-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>’s</td>\n",
       "      <td>’s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25-pound</td>\n",
       "      <td>25-pound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word     lemma\n",
       "0      This      this\n",
       "1        is        be\n",
       "2     Bruno     Bruno\n",
       "3         ,         ,\n",
       "4       and       and\n",
       "5        he    -PRON-\n",
       "6        ’s        ’s\n",
       "7         a         a\n",
       "8  25-pound  25-pound\n",
       "9       cat       cat"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_dataframe = pd.DataFrame(rows)\n",
    "lemma_dataframe.columns = ['word', 'lemma']\n",
    "lemma_dataframe.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the difference between the aggregated counts with and without the stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-PRON-          84\n",
       ",               34\n",
       ".               33\n",
       "be              28\n",
       "\"               27\n",
       "the             20\n",
       "\\n\\n            18\n",
       "a               15\n",
       "to              15\n",
       "and             12\n",
       "“                9\n",
       "of               8\n",
       "say              8\n",
       "have             8\n",
       "but              7\n",
       "in               7\n",
       "shelter          7\n",
       "do               7\n",
       "with             7\n",
       "like             6\n",
       "pet              6\n",
       "on               6\n",
       "not              6\n",
       "foster           6\n",
       "Bruno            6\n",
       "at               6\n",
       "water            5\n",
       "also             5\n",
       "if               5\n",
       "’s               5\n",
       "                ..\n",
       "trick            1\n",
       "(                1\n",
       "play             1\n",
       "big              1\n",
       "Rescue           1\n",
       "where            1\n",
       "Video            1\n",
       "teach            1\n",
       "out              1\n",
       "Wright           1\n",
       "apparently       1\n",
       "meet             1\n",
       "scratcher        1\n",
       "furry            1\n",
       "house            1\n",
       "11               1\n",
       "end              1\n",
       "take             1\n",
       "stay             1\n",
       "far              1\n",
       "side             1\n",
       "may              1\n",
       "top              1\n",
       "need             1\n",
       "mesh             1\n",
       "tempting         1\n",
       "adopter          1\n",
       "occasionally     1\n",
       "7-year           1\n",
       "lose             1\n",
       "Name: lemma, Length: 243, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_dataframe['lemma'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ",             34\n",
       ".             33\n",
       "I             20\n",
       "the           18\n",
       "\\n\\n          18\n",
       "a             15\n",
       "to            15\n",
       "my            13\n",
       "and           12\n",
       "“             12\n",
       "”             12\n",
       "\"             12\n",
       "is             9\n",
       "he             9\n",
       "you            8\n",
       "said           8\n",
       "of             8\n",
       "He             8\n",
       "shelter        7\n",
       "his            7\n",
       "in             7\n",
       "with           7\n",
       "but            7\n",
       "Bruno          6\n",
       "foster         6\n",
       "at             6\n",
       "was            6\n",
       "on             6\n",
       "like           5\n",
       "also           5\n",
       "              ..\n",
       "help           1\n",
       "really         1\n",
       "Adoption       1\n",
       "25-pound       1\n",
       "loved          1\n",
       "making         1\n",
       "very           1\n",
       "normal         1\n",
       "most           1\n",
       "simple         1\n",
       "No             1\n",
       "great          1\n",
       "sleep          1\n",
       "(              1\n",
       "may            1\n",
       "big            1\n",
       "gains          1\n",
       "polydactyl     1\n",
       "purring        1\n",
       "staying        1\n",
       "Rescue         1\n",
       "wand           1\n",
       "meshing        1\n",
       "Facebook       1\n",
       "extra          1\n",
       "where          1\n",
       "former         1\n",
       "right          1\n",
       "laying         1\n",
       "lose           1\n",
       "Name: word, Length: 288, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_dataframe['word'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Extracting Entities\n",
    "\n",
    "A common NLP task that might be useful is the extraction of **entities**. These include people, business, places, organizations and dates–for a full list of what entity types spacy is able to recognize out of the box, you can refer to the [documentation](https://spacy.io/api/annotation#named-entities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view them highlighted in the text by using `displacy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style = \"ent\",jupyter = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
